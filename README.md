# Mini AI Query System

**Project Overview**
- This project is a Retrieval-Augmented Generation (RAG) system. It lets you ask questions about your own PDF documents and get answers generated by a language model, with clear references to the source documents.
- The goal is to make it easy to search and understand your documents using natural language, with transparency about where the information comes from.

**How It Works**
- First, you place your PDF files in the `docs/` folder. The system is designed to automatically detect any new or changed PDFs every time it starts.
- When you launch the backend, it checks for changes in your documents. If there are new or updated files, it processes them, splits them into smaller chunks, and creates vector embeddings for each chunk using the OllamaEmbeddings model.
- These embeddings are stored in a FAISS vector database, which allows for fast and accurate similarity search when you ask a question.
- The backend is built with FastAPI and provides two main endpoints: `/query` for submitting questions and `/feedback` for collecting user 

## Setup Instructions

1. **Clone the repository** and navigate to the project directory.
2. **Install Python 3.11+** (recommended).
3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```
4. **Add your PDF files** to the `docs/` directory.
5. **Start the FastAPI server:**
   ```bash
   uvicorn frontend.api:app --reload
   ```
6. **Open the Web UI:**
   Open [http://localhost:8000/index.html](http://localhost:8000/index.html) in your browser to use the chatbot interface.

7. **(Optional) Access the API docs:**
   Open [http://localhost:8000/docs](http://localhost:8000/docs) in your browser.

## Libraries Used
- [FastAPI](https://fastapi.tiangolo.com/)
- [Pydantic](https://pydantic-docs.helpmanual.io/)
- [langchain](https://python.langchain.com/)
- [langchain_ollama](https://github.com/langchain-ai/langchain-ollama)
- [langchain_community](https://github.com/langchain-ai/langchain)
- [FAISS](https://github.com/facebookresearch/faiss)
- [OllamaEmbeddings]
- [ChatGroq]
- [PyPDF]

## Sample Queries to Test
Send a POST request to `/query` endpoint with JSON body:
```json
{
  "query": "What is the title of the document?",
  "role": "General"
}
```
Sample response:
```json
{
  "answer": "The title of the document is 'The Black Cat'.",
  "sources": ["The Black Cat Author Edgar Allan Poe.pdf"]
}
```

Send feedback to `/feedback` endpoint:
```json
{
  "query": "What is the title of the document?",
  "answer": "The Black Cat",
  "helpful": true,
  "user_comment": "Accurate answer."
}
```

## Notes on Limitations or Assumptions
- Only PDF files in the `docs/` directory are processed.
- The system automatically detects and indexes new or changed PDFs on restart.
- The vectorstore is cached for faster startup; cache is invalidated if PDFs change.
- Only basic role-based context is supported.
- The system is designed for demonstration and may not scale for large document sets or high concurrency.
- Ensure all dependencies are installed and compatible with your Python version.


